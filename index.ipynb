{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048e39db",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Sentiment:** When someone feels good about something, it's called a positive feeling, and when they feel bad about something, it's called a negative feeling, those feelings are \"sentiment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607bdadb",
   "metadata": {},
   "source": [
    "# OSEMN\n",
    "* Obtain\n",
    "* Scrub\n",
    "* Explore \n",
    "* Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd08c3",
   "metadata": {},
   "source": [
    "## 1. Obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c87d28",
   "metadata": {},
   "source": [
    "The Twitter Sentiment Analysis Dataset is a corpus of 1,578,627 classified tweets, with each tweet marked as 1 for positive sentiment and 0 for negative sentiment. The dataset is based on data from the University of Michigan Sentiment Analysis competition on Kaggle and the Twitter Sentiment Corpus by Niek Sanders. It is recommended to use 1/10 of the dataset for testing and the rest for training. The dataset has been used to achieve a 75% accuracy rate with a simple Naive Bayesian classification algorithm. The use of natural language processing can be helpful in extracting context and identifying features that contribute towards sentiment deduction. However, it is important to note that social informal communication, such as tweets, may not conform to grammatical rules and contain shortened words and overuse of punctuation. Despite these limitations, the dataset provides a good starting point for sentiment analysis modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635209e",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328324f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "\n",
    "# read the csv file to table\n",
    "df = pd.read_csv(\"/home/munyao/Desktop/flat_iron_school/Moringa/phase_4/NLP/Data/Sentiment Analysis Dataset.csv\", on_bad_lines='skip', index_col=0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574dddf",
   "metadata": {},
   "source": [
    "## Scrub\n",
    "* Removing stop words (words that are very common and do not add much meaning to the text)\n",
    "* Removing punctuation and special characters\n",
    "* Tokenizing the text (splitting it into words or phrases)\n",
    "* Stemming or lemmatizing the words (reducing them to their base form)\n",
    "* Removing URLs, mentions, or hashtags if you are working with social media data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361dec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531042c",
   "metadata": {},
   "source": [
    ">A function in python using the Natural Language Toolkit (NLTK) library to perform these text preprocessing steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e46e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the stopwords and lemmatizer data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the pre-processing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|@[^\\s]+|#\\S+', '', text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511da441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the dataset names, sizes, and sentiment scores\n",
    "dataset_names = ['Sentiment140', 'Kaggle']\n",
    "dataset_sizes = [1577269, 1343]\n",
    "dataset_scores = [0.2, 0.8] \n",
    "\n",
    "# Create a pandas DataFrame with the dataset information\n",
    "data = pd.DataFrame({'Dataset': dataset_names, 'Size': dataset_sizes, 'Score': dataset_scores})\n",
    "\n",
    "# Create a bubble chart using seaborn\n",
    "sns.scatterplot(data=data, x='Score', y='Dataset', size='Size', sizes=(100, 1000), alpha=0.7)\n",
    "plt.title('Sentiment and tweet count for each dataset')\n",
    "plt.xlabel('Sentiment score')\n",
    "plt.ylabel('Dataset')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77963797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing to the 'text' column\n",
    "df['ProcessedSentimentText'] = df['SentimentText'].apply(preprocess_text)\n",
    "\n",
    "# Show the processed DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3127a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the pre-processed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['ProcessedSentimentText'], df['Sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7beaa",
   "metadata": {},
   "source": [
    "## vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# example documents\n",
    "documents = X_train\n",
    "\n",
    "# create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# transform the documents into a sparse matrix using csr_matrix\n",
    "bow_representation = csr_matrix(vectorizer.fit_transform(documents))\n",
    "\n",
    "# print the shape of the sparse matrix\n",
    "print(bow_representation.shape)\n",
    "\n",
    "# print the sparse matrix in compressed sparse row format\n",
    "bow_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b75e6e",
   "metadata": {},
   "source": [
    "I analyze the text data using a tool called the Fourier transform. This tool helps us understand the different patterns and frequencies in the text. I use this information to figure out how people feel in the text. For example, we might find that certain patterns are associated with happy or sad feelings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ab1fc",
   "metadata": {},
   "source": [
    "**Fourier transform**\n",
    ">A mathematical technique that decomposes a time-domain signal into its constituent frequencies. It is named after Joseph Fourier, who discovered that any periodic waveform can be expressed as a sum of sine and cosine waves of different frequencies.\n",
    "\n",
    ">In mathematical terms, the Fourier transform of a continuous-time signal x(t) is defined as:\n",
    "\n",
    ">>X(f) = ∫x(t)e^(-j2πft)dt\n",
    "\n",
    "where X(f) is the frequency-domain representation of the signal x(t), and f is the frequency in hertz. The Fourier transform maps a function of time into a function of frequency.\n",
    "\n",
    ">The inverse Fourier transform, on the other hand, is used to recover the time-domain signal from its frequency-domain representation. It is defined as:\n",
    "\n",
    ">>x(t) = (1/T) ∫X(f)e^(j2πft)df\n",
    "\n",
    ">where T is the duration of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "# apply Fourier transform to the BOW representation\n",
    "fft_representation = fft(bow_sparse_matrix)\n",
    "\n",
    "# print the resulting Fourier coefficients\n",
    "print(fft_representation.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(freqs[np.argmax(X_freq, axis=1)], bins=20) # plot most prominent frequency for each document\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_env",
   "language": "python",
   "name": "sentiment_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
