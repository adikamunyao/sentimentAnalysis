{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048e39db",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Sentiment:** When someone feels good about something, it's called a positive feeling, and when they feel bad about something, it's called a negative feeling, those feelings are \"sentiment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607bdadb",
   "metadata": {},
   "source": [
    "# OSEMN\n",
    "* Obtain\n",
    "* Scrub\n",
    "* Explore \n",
    "* Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd08c3",
   "metadata": {},
   "source": [
    "# 1. Obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c87d28",
   "metadata": {},
   "source": [
    "The Twitter Sentiment Analysis Dataset is a corpus of 1,578,627 classified tweets, with each tweet marked as 1 for positive sentiment and 0 for negative sentiment. The dataset is based on data from the University of Michigan Sentiment Analysis competition on Kaggle and the Twitter Sentiment Corpus by Niek Sanders. It is recommended to use 1/10 of the dataset for testing and the rest for training. The dataset has been used to achieve a 75% accuracy rate with a simple Naive Bayesian classification algorithm. The use of natural language processing can be helpful in extracting context and identifying features that contribute towards sentiment deduction. However, it is important to note that social informal communication, such as tweets, may not conform to grammatical rules and contain shortened words and overuse of punctuation. Despite these limitations, the dataset provides a good starting point for sentiment analysis modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99c977",
   "metadata": {},
   "source": [
    "#### Load neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a17387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "np.random.seed(0)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635209e",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328324f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment SentimentSource  \\\n",
       "ItemID                              \n",
       "1               0    Sentiment140   \n",
       "2               0    Sentiment140   \n",
       "3               1    Sentiment140   \n",
       "4               0    Sentiment140   \n",
       "5               0    Sentiment140   \n",
       "6               0    Sentiment140   \n",
       "7               1    Sentiment140   \n",
       "\n",
       "                                            SentimentText  \n",
       "ItemID                                                     \n",
       "1                            is so sad for my APL frie...  \n",
       "2                          I missed the New Moon trail...  \n",
       "3                                 omg its already 7:30 :O  \n",
       "4                 .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "5                i think mi bf is cheating on me!!!   ...  \n",
       "6                       or i just worry too much?          \n",
       "7                      Juuuuuuuuuuuuuuuuussssst Chillin!!  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv file to table\n",
    "df = pd.read_csv(\"/home/munyao/Desktop/flat_iron_school/Moringa/phase_4/NLP/Data/Sentiment Analysis Dataset.csv\", on_bad_lines='skip', index_col=0)\n",
    "\n",
    "# preview first 7 rows of dataset.\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574dddf",
   "metadata": {},
   "source": [
    "# 2. Scrub\n",
    "* Convert the text to lowercase: This is done so that the analysis is not case-sensitive.\n",
    "\n",
    "* Remove URLs, mentions, and hashtags: These are typically not relevant to the analysis and can be removed.\n",
    "\n",
    "* Remove punctuation and special characters: These can also be removed as they do not add any value to the analysis.\n",
    "\n",
    "* Tokenize the text into words: This splits the text into individual words, which can then be analyzed separately.\n",
    "\n",
    "* Remove stop words: These are common words such as \"the\", \"and\", and \"a\" that do not typically carry much meaning and can be removed.\n",
    "\n",
    "* Perform lemmatization: This reduces words to their base form, so that variations of the same word are treated as the same (e.g. \"walks\", \"walked\", and \"walking\" all become \"walk\").\n",
    "\n",
    "* Join the tokens back into a string: This reassembles the processed words into a single string that can be used for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea2bea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1578612, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224d571",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning and Normalizing \n",
    "1. Convert the text to lowercase: This is done so that the analysis is not case-sensitive.\n",
    "2. Remove URLs, mentions, and hashtags: These are typically not relevant to the analysis and can be removed.\n",
    "3. Remove punctuation and special characters: These can also be removed as they do not add any value to the analysis.\n",
    "4. Tokenize the text into words: This splits the text into individual words, which can then be analyzed separately.\n",
    "5. Remove stop words: These are common words such as \"the\", \"and\", and \"a\" that do not typically carry much meaning and can be removed.\n",
    "6. Perform lemmatization: This reduces words to their base form, so that variations of the same word are treated as the same (e.g. \"walks\", \"walked\", and \"walking\" all become \"walk\").\n",
    "7. Join the tokens back into a string: This reassembles the processed words into a single string that can be used for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38e2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pre-processing function\n",
    "def preprocess_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|@[^\\s]+|#\\S+', '', text)\n",
    "\n",
    "    # remove excessive letters\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "    # remove punctuation and special characters\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a75055",
   "metadata": {},
   "source": [
    "### 2.2 Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb75b62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>ProcessedSentimentText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>missed new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg already 730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>omgaga im soo im gunna cry ive dentist since 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>think mi bf cheating tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "      <td>worry much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "      <td>juusst chillin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment SentimentSource  \\\n",
       "ItemID                              \n",
       "1               0    Sentiment140   \n",
       "2               0    Sentiment140   \n",
       "3               1    Sentiment140   \n",
       "4               0    Sentiment140   \n",
       "5               0    Sentiment140   \n",
       "6               0    Sentiment140   \n",
       "7               1    Sentiment140   \n",
       "\n",
       "                                            SentimentText  \\\n",
       "ItemID                                                      \n",
       "1                            is so sad for my APL frie...   \n",
       "2                          I missed the New Moon trail...   \n",
       "3                                 omg its already 7:30 :O   \n",
       "4                 .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "5                i think mi bf is cheating on me!!!   ...   \n",
       "6                       or i just worry too much?           \n",
       "7                      Juuuuuuuuuuuuuuuuussssst Chillin!!   \n",
       "\n",
       "                                   ProcessedSentimentText  \n",
       "ItemID                                                     \n",
       "1                                          sad apl friend  \n",
       "2                                 missed new moon trailer  \n",
       "3                                         omg already 730  \n",
       "4       omgaga im soo im gunna cry ive dentist since 1...  \n",
       "5                                 think mi bf cheating tt  \n",
       "6                                              worry much  \n",
       "7                                          juusst chillin  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply pre-processing to the 'text' column\n",
    "df['ProcessedSentimentText'] = df['SentimentText'].apply(preprocess_text)\n",
    "\n",
    "# preview the processed data\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b75e6e",
   "metadata": {},
   "source": [
    "I analyze the text data using a tool called the Fourier transform. This tool helps us understand the different patterns and frequencies in the text. I use this information to figure out how people feel in the text. For example, we might find that certain patterns are associated with happy or sad feelings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03311ff2",
   "metadata": {},
   "source": [
    "# 3. Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520a3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# define a function to plot the power spectrum\n",
    "def plot_power_spectrum(power_spectrum, title):\n",
    "    freq = np.fft.fftfreq(len(power_spectrum))\n",
    "    plt.plot(freq, power_spectrum)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_env",
   "language": "python",
   "name": "sentiment_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
